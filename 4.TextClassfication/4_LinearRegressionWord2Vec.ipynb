{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.LinearRegressionWord2Vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk32uwAW7DTG"
      },
      "source": [
        "reference : reference : https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBbtuZIj6sjC"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJB_esv97Rxt",
        "outputId": "57593582-9836-4a05-f950-e8d131c7bee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tQJ4N3v7ZaQ"
      },
      "source": [
        "DATA_IN_PATH = 'gdrive/My Drive/RogerHeederer/NLP_KR/4.TextClassfication_heederer/data_in/'\n",
        "DATA_OUT_PATH = 'gdrive/My Drive/RogerHeederer/NLP_KR/4.TextClassfication_heederer/data_out/'\n",
        "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "TEST_SPLIT = 0.2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95Xe3E067v9V"
      },
      "source": [
        "#전처리된 텍스트를 활용하여 벡터화 한다. 전처리한 넘파이 배열은 워드투백에서 사용하지 않는다.\n",
        "train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMfFVW7u70Lg"
      },
      "source": [
        "reviews = list(train_data['review'])\n",
        "sentiments = list(train_data['sentiment'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fWD-lyN8JR-"
      },
      "source": [
        "sentences = []\n",
        "for review in reviews: #전체 리뷰를 띄어쓰기 단위로 잘라서 단어 리스트로 만든다\n",
        "  sentences.append(review.split())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GixTqXp98UAK",
        "outputId": "1ab68970-fe83-4cc8-faa7-6dbdaee1db61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sentences[1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['classic',\n",
              " 'war',\n",
              " 'worlds',\n",
              " 'timothy',\n",
              " 'hines',\n",
              " 'entertaining',\n",
              " 'film',\n",
              " 'obviously',\n",
              " 'goes',\n",
              " 'great',\n",
              " 'effort',\n",
              " 'lengths',\n",
              " 'faithfully',\n",
              " 'recreate',\n",
              " 'h',\n",
              " 'g',\n",
              " 'wells',\n",
              " 'classic',\n",
              " 'book',\n",
              " 'mr',\n",
              " 'hines',\n",
              " 'succeeds',\n",
              " 'watched',\n",
              " 'film',\n",
              " 'appreciated',\n",
              " 'fact',\n",
              " 'standard',\n",
              " 'predictable',\n",
              " 'hollywood',\n",
              " 'fare',\n",
              " 'comes',\n",
              " 'every',\n",
              " 'year',\n",
              " 'e',\n",
              " 'g',\n",
              " 'spielberg',\n",
              " 'version',\n",
              " 'tom',\n",
              " 'cruise',\n",
              " 'slightest',\n",
              " 'resemblance',\n",
              " 'book',\n",
              " 'obviously',\n",
              " 'everyone',\n",
              " 'looks',\n",
              " 'different',\n",
              " 'things',\n",
              " 'movie',\n",
              " 'envision',\n",
              " 'amateur',\n",
              " 'critics',\n",
              " 'look',\n",
              " 'criticize',\n",
              " 'everything',\n",
              " 'others',\n",
              " 'rate',\n",
              " 'movie',\n",
              " 'important',\n",
              " 'bases',\n",
              " 'like',\n",
              " 'entertained',\n",
              " 'people',\n",
              " 'never',\n",
              " 'agree',\n",
              " 'critics',\n",
              " 'enjoyed',\n",
              " 'effort',\n",
              " 'mr',\n",
              " 'hines',\n",
              " 'put',\n",
              " 'faithful',\n",
              " 'h',\n",
              " 'g',\n",
              " 'wells',\n",
              " 'classic',\n",
              " 'novel',\n",
              " 'found',\n",
              " 'entertaining',\n",
              " 'made',\n",
              " 'easy',\n",
              " 'overlook',\n",
              " 'critics',\n",
              " 'perceive',\n",
              " 'shortcomings']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DppXHHld9lta"
      },
      "source": [
        "#word2Vec 벡터화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcVyhUZt9Km6"
      },
      "source": [
        "#학습 시 필요한 하이퍼파라미터 설정\n",
        "num_features = 300    # 각 단어에 대해 임베딩된 벡터의 차원\n",
        "min_word_count = 40   # 적은 빈도수의 단어들은 학습하지 않는다. 40이상 빈도를 가지는 단어만 학습\n",
        "num_workers = 4       # 모델 학습 시 학습을 위한 프로세스 개수를 지정한다.\n",
        "context = 10          # 워드투백을 수행하기 위한 컨텍스트 윈도 크기 지정\n",
        "downsampling = 1e-3   # 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율 지정. 보통 0.001로 성능이 좋다"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuQs41rc-hs-",
        "outputId": "453557d4-6619-4082-a6c4-9ccd3e556f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "#위 파라미터를 이용해서 워드투백 학습시작\n",
        "!pip install gensim"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.2.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeNn7UHPAUje"
      },
      "source": [
        "# 워드투벡 학습 과정 진행 상황을 확인해보기 위해 다음과 같은 로깅 기능 사용\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "#로그 수준을 INFO에 맞추어 워드투벡 학습과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 보여주게 된다"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUUXcnqkA8Yy",
        "outputId": "80c5e8a1-2a6a-4558-d473-b81d30625d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from gensim.models import word2vec\n",
        "print(\"Training model...\") #각각의 단어 리스트들에 대한 벡터값을 가지고 있는 모델이 된다.\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features,\n",
        "                          min_count = min_word_count, window=context, sample=downsampling)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-13 16:29:47,123 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2020-10-13 16:29:47,134 : INFO : collecting all words and their counts\n",
            "2020-10-13 16:29:47,135 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-10-13 16:29:47,435 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n",
            "2020-10-13 16:29:47,722 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
            "2020-10-13 16:29:47,874 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
            "2020-10-13 16:29:47,875 : INFO : Loading a fresh vocabulary\n",
            "2020-10-13 16:29:47,921 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
            "2020-10-13 16:29:47,922 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
            "2020-10-13 16:29:47,964 : INFO : deleting the raw counts dictionary of 74065 items\n",
            "2020-10-13 16:29:47,967 : INFO : sample=0.001 downsamples 30 most-common words\n",
            "2020-10-13 16:29:47,969 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
            "2020-10-13 16:29:47,995 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
            "2020-10-13 16:29:47,995 : INFO : resetting layer weights\n",
            "2020-10-13 16:29:49,612 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2020-10-13 16:29:50,631 : INFO : EPOCH 1 - PROGRESS: at 12.22% examples, 302226 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:29:51,648 : INFO : EPOCH 1 - PROGRESS: at 25.30% examples, 315329 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:29:52,665 : INFO : EPOCH 1 - PROGRESS: at 38.11% examples, 314987 words/s, in_qsize 8, out_qsize 2\n",
            "2020-10-13 16:29:53,670 : INFO : EPOCH 1 - PROGRESS: at 51.68% examples, 320768 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:29:54,675 : INFO : EPOCH 1 - PROGRESS: at 64.00% examples, 317653 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:29:55,690 : INFO : EPOCH 1 - PROGRESS: at 77.34% examples, 319181 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:29:56,745 : INFO : EPOCH 1 - PROGRESS: at 90.34% examples, 317255 words/s, in_qsize 6, out_qsize 1\n",
            "2020-10-13 16:29:57,350 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-10-13 16:29:57,377 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-10-13 16:29:57,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-10-13 16:29:57,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-10-13 16:29:57,414 : INFO : EPOCH - 1 : training on 2988089 raw words (2494292 effective words) took 7.8s, 319980 effective words/s\n",
            "2020-10-13 16:29:58,427 : INFO : EPOCH 2 - PROGRESS: at 11.87% examples, 295791 words/s, in_qsize 8, out_qsize 1\n",
            "2020-10-13 16:29:59,439 : INFO : EPOCH 2 - PROGRESS: at 25.30% examples, 315349 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:00,475 : INFO : EPOCH 2 - PROGRESS: at 38.11% examples, 314186 words/s, in_qsize 6, out_qsize 1\n",
            "2020-10-13 16:30:01,477 : INFO : EPOCH 2 - PROGRESS: at 51.68% examples, 320305 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:02,519 : INFO : EPOCH 2 - PROGRESS: at 64.76% examples, 318325 words/s, in_qsize 7, out_qsize 1\n",
            "2020-10-13 16:30:03,536 : INFO : EPOCH 2 - PROGRESS: at 77.67% examples, 318213 words/s, in_qsize 6, out_qsize 1\n",
            "2020-10-13 16:30:04,560 : INFO : EPOCH 2 - PROGRESS: at 90.69% examples, 317797 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:05,195 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-10-13 16:30:05,210 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-10-13 16:30:05,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-10-13 16:30:05,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-10-13 16:30:05,231 : INFO : EPOCH - 2 : training on 2988089 raw words (2494072 effective words) took 7.8s, 319381 effective words/s\n",
            "2020-10-13 16:30:06,295 : INFO : EPOCH 3 - PROGRESS: at 12.52% examples, 297255 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:07,305 : INFO : EPOCH 3 - PROGRESS: at 25.30% examples, 307812 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:08,325 : INFO : EPOCH 3 - PROGRESS: at 38.07% examples, 310808 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:09,336 : INFO : EPOCH 3 - PROGRESS: at 50.69% examples, 310998 words/s, in_qsize 8, out_qsize 0\n",
            "2020-10-13 16:30:10,384 : INFO : EPOCH 3 - PROGRESS: at 64.06% examples, 312127 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:11,388 : INFO : EPOCH 3 - PROGRESS: at 78.01% examples, 317760 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:12,403 : INFO : EPOCH 3 - PROGRESS: at 90.36% examples, 315476 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:13,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-10-13 16:30:13,057 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-10-13 16:30:13,060 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-10-13 16:30:13,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-10-13 16:30:13,086 : INFO : EPOCH - 3 : training on 2988089 raw words (2494588 effective words) took 7.8s, 317883 effective words/s\n",
            "2020-10-13 16:30:14,106 : INFO : EPOCH 4 - PROGRESS: at 12.54% examples, 310456 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:15,124 : INFO : EPOCH 4 - PROGRESS: at 25.30% examples, 313373 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:16,156 : INFO : EPOCH 4 - PROGRESS: at 38.43% examples, 315992 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:17,191 : INFO : EPOCH 4 - PROGRESS: at 51.35% examples, 315112 words/s, in_qsize 6, out_qsize 1\n",
            "2020-10-13 16:30:18,191 : INFO : EPOCH 4 - PROGRESS: at 65.10% examples, 320009 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:19,206 : INFO : EPOCH 4 - PROGRESS: at 77.67% examples, 318373 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:20,248 : INFO : EPOCH 4 - PROGRESS: at 91.02% examples, 318285 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:20,828 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-10-13 16:30:20,862 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-10-13 16:30:20,866 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-10-13 16:30:20,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-10-13 16:30:20,881 : INFO : EPOCH - 4 : training on 2988089 raw words (2494530 effective words) took 7.8s, 320337 effective words/s\n",
            "2020-10-13 16:30:21,937 : INFO : EPOCH 5 - PROGRESS: at 12.22% examples, 293745 words/s, in_qsize 8, out_qsize 2\n",
            "2020-10-13 16:30:22,933 : INFO : EPOCH 5 - PROGRESS: at 25.30% examples, 311157 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:23,961 : INFO : EPOCH 5 - PROGRESS: at 38.42% examples, 314907 words/s, in_qsize 6, out_qsize 1\n",
            "2020-10-13 16:30:24,983 : INFO : EPOCH 5 - PROGRESS: at 52.00% examples, 319219 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:25,988 : INFO : EPOCH 5 - PROGRESS: at 65.10% examples, 319750 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:27,011 : INFO : EPOCH 5 - PROGRESS: at 77.70% examples, 317751 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:28,076 : INFO : EPOCH 5 - PROGRESS: at 91.36% examples, 317887 words/s, in_qsize 7, out_qsize 0\n",
            "2020-10-13 16:30:28,625 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-10-13 16:30:28,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-10-13 16:30:28,647 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-10-13 16:30:28,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-10-13 16:30:28,649 : INFO : EPOCH - 5 : training on 2988089 raw words (2494227 effective words) took 7.8s, 321356 effective words/s\n",
            "2020-10-13 16:30:28,654 : INFO : training on a 14940445 raw words (12471709 effective words) took 39.0s, 319453 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf5pz-nUBo7D",
        "outputId": "46d1ed2a-cdb6-46d0-dee5-6c4ee9a07138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담아서 나중에 참고해보자\n",
        "#Word2Vec.load()를 통해서 모델을 다시 불러와 사용할 수 있다.\n",
        "model_name = \"gdrive/My Drive/RogerHeederer/NLP_KR/4.TextClassfication_heederer/data_out/300features_40minwords_10context\"\n",
        "model.save(model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-13 12:57:11,494 : INFO : saving Word2Vec object under gdrive/My Drive/RogerHeederer/NLP_KR/4.TextClassfication_heederer/data_out/300features_40minwords_10context, separately None\n",
            "2020-10-13 12:57:11,496 : INFO : not storing attribute vectors_norm\n",
            "2020-10-13 12:57:11,500 : INFO : not storing attribute cum_table\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-10-13 12:57:11,896 : INFO : saved gdrive/My Drive/RogerHeederer/NLP_KR/4.TextClassfication_heederer/data_out/300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUDiSV8TzERg"
      },
      "source": [
        "이제 만들어진 word2vec 모델을 활용해서 선형 회귀 모델을 학습해보겠다. 우선 학습을 위해서는 리뷰들을 같은 형태의 입력값으로 만들어야 한다. 리뷰마다 단어 개수가 모두 다르기 때문에, 입력값을 하나의 형태로 만드는 방법이 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbMsG0l_1uPk"
      },
      "source": [
        "문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--hqkCMLCBBG"
      },
      "source": [
        "#words : 단어 모음인 하나의 리뷰가 들어감\n",
        "#model : 임베딩된 word2vec 모델을 넣는 곳\n",
        "#num_features : word2vec으로 임베딩할 때 정했던 벡터의 차원수\n",
        "def get_features(words, model, num_features):\n",
        "  feature_vector = np.zeros((num_features), dtype=np.float32) #출력 벡터 초기화\n",
        "  num_words=0\n",
        "  index2word_set = set(model.wv.index2word) #어휘 사전 준비 / 문장의 단어가 해당 모델 단어 사전에 속하는지 확인하기 위해\n",
        "                                          \n",
        "  for w in words: \n",
        "    if w in index2word_set: #리뷰의 단어 하나 하나를 word2vec에 있는 단어인지 체크 한다.\n",
        "      num_words += 1 #있다면 단어 숫자를 1 올려주고\n",
        "      feature_vector = np.add(feature_vector, model[w]) # word2vec에서 그 단어의 벡터값을 가져와 더해준다.\n",
        "\n",
        "  feature_vector = np.divide(feature_vector, num_words) #단어 벡터값들의 합을, 단어 개수만큼 나눠준다(평균)\n",
        "  return feature_vector"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w8NBl242S8O"
      },
      "source": [
        "# 전체 리뷰에 대해 각 리뷰의 평균 벡터를 구하는 함수 정의\n",
        "# reviews: 학습 데이터인 전체 리뷰 데이터를 입력받는 인자\n",
        "\n",
        "def get_dataset(reviews, model, num_features):\n",
        "  dataset = list()\n",
        "\n",
        "  for s in reviews:\n",
        "    dataset.append(get_features(s, model, num_features))\n",
        "  \n",
        "  reviewFeatureVecs = np.stack(dataset) #각 리뷰를 라인 바이 라인으로 벡터를 구해 쌓아놓는다.\n",
        "  return reviewFeatureVecs"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-f-XMST3cK2",
        "outputId": "9404b3dc-857a-4961-d7c0-530df4f30354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#이제 위 두함수를 사용해서 실제 학습에 사용될 입력값을 만들어보자\n",
        "test_data_vecs = get_dataset(sentences, model, num_features)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBKTjPaE4Erv",
        "outputId": "f1b1d27a-054e-425f-8655-e10adaea1130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "test_data_vecs"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.23184285, -0.1198298 , -0.16366903, ...,  0.08386903,\n",
              "        -0.10069377, -0.09224693],\n",
              "       [-0.06559019, -0.23833539, -0.30501917, ..., -0.28208685,\n",
              "        -0.07988952,  0.02504665],\n",
              "       [-0.08757322,  0.04175428,  0.06635428, ...,  0.14136074,\n",
              "        -0.04709114, -0.11670062],\n",
              "       ...,\n",
              "       [ 0.10825901, -0.11592989, -0.3537106 , ...,  0.13039844,\n",
              "        -0.06310982, -0.07527832],\n",
              "       [ 0.0627091 , -0.2710136 , -0.02810978, ...,  0.11511368,\n",
              "        -0.06486975, -0.17145921],\n",
              "       [ 0.00709259, -0.30465266, -0.26342863, ...,  0.0161927 ,\n",
              "         0.08599084,  0.02401569]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUNrsvPo3wuU"
      },
      "source": [
        "이제 만들어진 데이터를 가지고 학습 데이터, 검증 데이터를 나눠보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSWFxU4d3toK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "X = test_data_vecs\n",
        "y = np.array(sentiments)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGKvEKTh4dOv",
        "outputId": "89d2bb55-33a3-4696-b3ab-50de3b241c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape, X_test.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000, 300), (5000, 300))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxCYXlt64fXk",
        "outputId": "16cb1713-358f-413b-c4a7-7285c2c1d629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train.shape, y_test.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20000,), (5000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YSRFd9F4j_2",
        "outputId": "4b3bf032-1ccc-4ee4-9221-5b1ff486b4a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "#이제 word2vec을 적용한 리뷰들을 모델에 투입하여 학습시켜보자\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lgs = LogisticRegression(class_weight='balanced')\n",
        "lgs.fit(X_train, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
              "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
              "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJBL2ZIX43FH",
        "outputId": "8e7a5508-b6be-4629-f2e8-2ef761686cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "predicted = lgs.predict(X_test)\n",
        "from sklearn import metrics\n",
        "\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test, (lgs.predict_proba(X_test)[:, 1]))\n",
        "auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "print(\"------------\")\n",
        "print(\"Accuracy: %f\" % lgs.score(X_test, y_test))  #checking the accuracy\n",
        "print(\"Precision: %f\" % metrics.precision_score(y_test, predicted))\n",
        "print(\"Recall: %f\" % metrics.recall_score(y_test, predicted))\n",
        "print(\"F1-Score: %f\" % metrics.f1_score(y_test, predicted))\n",
        "print(\"AUC: %f\" % auc)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------\n",
            "Accuracy: 0.864200\n",
            "Precision: 0.857420\n",
            "Recall: 0.876141\n",
            "F1-Score: 0.866680\n",
            "AUC: 0.934073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i110Tw875n5O"
      },
      "source": [
        "이제 캐글에 제출하기 위한 데이터 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSWNSkN5U_6"
      },
      "source": [
        "#전처리 끝난 평가 데이터 가져오기\n",
        "TEST_CLEAN_DATA = 'test_clean.csv'\n",
        "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
        "test_review = list(test_data['review'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7FV-wRn6h7s",
        "outputId": "a21be1fd-e579-4840-9133-27ba3c572808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "test_data.head(5)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>naturally film main themes mortality nostalgia...</td>\n",
              "      <td>\"12311_10\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>movie disaster within disaster film full great...</td>\n",
              "      <td>\"8348_2\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>movie kids saw tonight child loved one point k...</td>\n",
              "      <td>\"5828_4\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>afraid dark left impression several different ...</td>\n",
              "      <td>\"7186_2\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>accurate depiction small time mob life filmed ...</td>\n",
              "      <td>\"12128_7\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review          id\n",
              "0  naturally film main themes mortality nostalgia...  \"12311_10\"\n",
              "1  movie disaster within disaster film full great...    \"8348_2\"\n",
              "2  movie kids saw tonight child loved one point k...    \"5828_4\"\n",
              "3  afraid dark left impression several different ...    \"7186_2\"\n",
              "4  accurate depiction small time mob life filmed ...   \"12128_7\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4CRD80J6j1P",
        "outputId": "97a1b73f-e125-400c-907c-c58114e7f820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#평가 데이터 역시 학습 데이터 처럼 단어의 리스트로 만들어야 한다\n",
        "test_sentences = list()\n",
        "for review in test_review:\n",
        "  test_sentences.append(review.split())\n",
        "\n",
        "test_sentences[1]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['movie',\n",
              " 'disaster',\n",
              " 'within',\n",
              " 'disaster',\n",
              " 'film',\n",
              " 'full',\n",
              " 'great',\n",
              " 'action',\n",
              " 'scenes',\n",
              " 'meaningful',\n",
              " 'throw',\n",
              " 'away',\n",
              " 'sense',\n",
              " 'reality',\n",
              " 'let',\n",
              " 'see',\n",
              " 'word',\n",
              " 'wise',\n",
              " 'lava',\n",
              " 'burns',\n",
              " 'steam',\n",
              " 'burns',\n",
              " 'stand',\n",
              " 'next',\n",
              " 'lava',\n",
              " 'diverting',\n",
              " 'minor',\n",
              " 'lava',\n",
              " 'flow',\n",
              " 'difficult',\n",
              " 'let',\n",
              " 'alone',\n",
              " 'significant',\n",
              " 'one',\n",
              " 'scares',\n",
              " 'think',\n",
              " 'might',\n",
              " 'actually',\n",
              " 'believe',\n",
              " 'saw',\n",
              " 'movie',\n",
              " 'even',\n",
              " 'worse',\n",
              " 'significant',\n",
              " 'amount',\n",
              " 'talent',\n",
              " 'went',\n",
              " 'making',\n",
              " 'film',\n",
              " 'mean',\n",
              " 'acting',\n",
              " 'actually',\n",
              " 'good',\n",
              " 'effects',\n",
              " 'average',\n",
              " 'hard',\n",
              " 'believe',\n",
              " 'somebody',\n",
              " 'read',\n",
              " 'scripts',\n",
              " 'allowed',\n",
              " 'talent',\n",
              " 'wasted',\n",
              " 'guess',\n",
              " 'suggestion',\n",
              " 'would',\n",
              " 'movie',\n",
              " 'start',\n",
              " 'tv',\n",
              " 'look',\n",
              " 'away',\n",
              " 'like',\n",
              " 'train',\n",
              " 'wreck',\n",
              " 'awful',\n",
              " 'know',\n",
              " 'coming',\n",
              " 'watch',\n",
              " 'look',\n",
              " 'away',\n",
              " 'spend',\n",
              " 'time',\n",
              " 'meaningful',\n",
              " 'content']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo2P1i4p7CN6",
        "outputId": "be18ea6d-12d9-4e30-ed21-8af5560e174f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#이제 워드투벡을 적용해서 임베딩된 벡터값으로 만들어보자\n",
        "#이전에 만들어놓은 워드투벡 모델을 적용해서, 위의 각 단어들을 벡터로 만들어 테스트 리뷰의 특징값을 만든다\n",
        "test_data_vecs = get_dataset(test_sentences, model, num_features)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHRX4aoU7X96"
      },
      "source": [
        "#이제 이 워드투벡화 된 평가데이터를 가지고 기학습시킨 로지스텍 모델에 적용해보자\n",
        "#이 평가 데이터는 정답 라벨을 가지고 있지 않기 때문에, 예측한 값을 캐글에 제출한 후 성능 점수를 피드백 받는다\n",
        "test_predicted = lgs.predict(test_data_vecs)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB6H8oe68pcO"
      },
      "source": [
        "ids = list(test_data['id'])\n",
        "\n",
        "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ddqk4vd8rQe",
        "outputId": "ead7c041-dd4e-433c-a229-07717855a29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "answer_dataset"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"12311_10\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"8348_2\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"5828_4\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"7186_2\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"12128_7\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>\"2155_10\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>\"59_10\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>\"2531_1\"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>\"7772_8\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>\"11465_10\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               id  sentiment\n",
              "0      \"12311_10\"          1\n",
              "1        \"8348_2\"          0\n",
              "2        \"5828_4\"          1\n",
              "3        \"7186_2\"          0\n",
              "4       \"12128_7\"          1\n",
              "...           ...        ...\n",
              "24995   \"2155_10\"          1\n",
              "24996     \"59_10\"          1\n",
              "24997    \"2531_1\"          0\n",
              "24998    \"7772_8\"          1\n",
              "24999  \"11465_10\"          1\n",
              "\n",
              "[25000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjKw2eTc70hs"
      },
      "source": [
        "DATA_OUT_PATH = 'gdrive/My Drive/RogerHeederer/NLP_KR/4.TextClassfication_heederer/data_out/'\n",
        "\n",
        "if not os.path.exists(DATA_OUT_PATH):\n",
        "    os.makedirs(DATA_OUT_PATH)\n",
        "\n",
        "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F4njl7j8_IV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}